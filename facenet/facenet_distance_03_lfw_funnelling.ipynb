{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Using FaceNet with distance metrics and LFW\n",
    "\n",
    "This notebook tests facenet-pytorch on LFW images aligned with deep funneling."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from facenet_pytorch import MTCNN, InceptionResnetV1, fixed_image_standardization, training, extract_face\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler, SequentialSampler\n",
    "from torch import optim\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import torch.nn.functional as F\n",
    "from copy import deepcopy\n",
    "import shutil\n",
    "from sklearn.metrics import accuracy_score"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "data_dir = 'data/LFW/lfw-deepfunneled'\n",
    "pairs_path = 'data/LFW/pairs.txt'\n",
    "\n",
    "batch_size = 16\n",
    "epochs = 15\n",
    "workers = 0 if os.name == 'nt' else 8"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "#Needed to rotate files based on exif. Default loader strips exif, so can't do in transform step. PIL files don't keep exif when made from MPO file\n",
    "\n",
    "def exif_rotate_pil_loader(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        image = Image.open(f)\n",
    "        image = reorient_image(image)\n",
    "        image = image.convert('RGB')  #replicates pil_loader from torchvision. Copies to convert to PIL\n",
    "    return image\n",
    "\n",
    "\n",
    "def reorient_image(im):\n",
    "    try:\n",
    "        image_exif = im._getexif()\n",
    "        image_orientation = image_exif[274]\n",
    "        if image_orientation in (2, '2'):\n",
    "            return im.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "        elif image_orientation in (3, '3'):\n",
    "            return im.transpose(Image.ROTATE_180)\n",
    "        elif image_orientation in (4, '4'):\n",
    "            return im.transpose(Image.FLIP_TOP_BOTTOM)\n",
    "        elif image_orientation in (5, '5'):\n",
    "            return im.transpose(Image.ROTATE_90).transpose(Image.FLIP_TOP_BOTTOM)\n",
    "        elif image_orientation in (6, '6'):\n",
    "            return im.transpose(Image.ROTATE_270)\n",
    "        elif image_orientation in (7, '7'):\n",
    "            return im.transpose(Image.ROTATE_270).transpose(Image.FLIP_TOP_BOTTOM)\n",
    "        elif image_orientation in (8, '8'):\n",
    "            return im.transpose(Image.ROTATE_90)\n",
    "        else:\n",
    "            return im\n",
    "    except (KeyError, AttributeError, TypeError, IndexError):\n",
    "        return im"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "class MTCNN_w_batch_extract(MTCNN):\n",
    "    \"\"\"custom class that includes addtional methods to allow for easier separation of detection,\n",
    "        selection and extraction.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def select_boxes(self, all_boxes, all_probs, all_points, method='probability', threshold=.9):\n",
    "        \"\"\"Selects a single box from multiple for a given image using one of multiple heuristics.\n",
    "        Arguments:\n",
    "                batch_boxes {np.ndarray} -- Nx4 ndarray of bounding boxes for N detected faces (output from self.detect)\n",
    "                batch_probs {list} -- Length N list of probalities for N detected faces. (output from self.detect)\n",
    "        Keyword Arguments:\n",
    "                method {str} -- Which heuristic to use for selection:\n",
    "                    \"probability\": highest probability selected\n",
    "                    \"largest\": largest box selected\n",
    "                    \"largest_over_theshold\": largest box over a certain probability threshold selected\n",
    "                threshold {float} -- theshold for \"largest_over_threshold\" method\n",
    "\n",
    "        Returns:\n",
    "                tuple(numpy.ndarray, numpy.ndarray) -- Ix4 ndarray of bounding boxes for I images. Ix0 array of probabilities for each box\n",
    "        \"\"\"\n",
    "        selected_boxes, selected_probs, selected_points = [], [], []\n",
    "        for boxes, points, probs in zip(all_boxes, all_points, all_probs):\n",
    "            boxes = np.array(boxes)\n",
    "            probs = np.array(probs)\n",
    "            points = np.array(points)\n",
    "            if len(boxes) == 0:\n",
    "                selected_boxes.append(None)\n",
    "                selected_probs.append([None])\n",
    "                selected_points.append(None)\n",
    "                continue\n",
    "            elif method == 'largest':\n",
    "                box_order = np.argsort((boxes[:, 2] - boxes[:, 0]) * (boxes[:, 3] - boxes[:, 1]))[::-1]\n",
    "            elif method == 'probability':\n",
    "                box_order = np.argsort(probs)[::-1]\n",
    "            elif method == 'largest_over_threshold':\n",
    "                box_mask = probs > threshold\n",
    "                boxes = boxes[box_mask]\n",
    "                box_order = np.argsort((boxes[:, 2] - boxes[:, 0]) * (boxes[:, 3] - boxes[:, 1]))[::-1]\n",
    "                if sum(box_mask) == 0:\n",
    "                    selected_boxes.append(None)\n",
    "                    selected_probs.append([None])\n",
    "                    selected_points.append(None)\n",
    "                    continue\n",
    "\n",
    "            box = boxes[box_order][[0]]\n",
    "            prob = probs[box_order][[0]]\n",
    "            point = points[box_order][[0]]\n",
    "            selected_boxes.append(box)\n",
    "            selected_probs.append(prob)\n",
    "            selected_points.append(point)\n",
    "\n",
    "        selected_boxes = np.array(selected_boxes)\n",
    "        selected_probs = np.array(selected_probs)\n",
    "        selected_points = np.array(selected_points)\n",
    "\n",
    "        return selected_boxes, selected_probs, selected_points\n",
    "\n",
    "    def extract(self, img, batch_boxes, save_path):\n",
    "        #Determine if a batch or single image was passed\n",
    "        batch_mode = True\n",
    "        if not isinstance(img, (list, tuple)) and not (isinstance(img, np.ndarray) and len(img.shape) == 4):\n",
    "            img = [img]\n",
    "            batch_boxes = [batch_boxes]\n",
    "            batch_mode = False\n",
    "\n",
    "        #Parse save path(s)\n",
    "        if save_path is not None:\n",
    "            if isinstance(save_path, str):\n",
    "                save_path = [save_path]\n",
    "        else:\n",
    "            save_path = [None for _ in range(len(img))]\n",
    "\n",
    "        #Process all bounding boxes\n",
    "        faces = []\n",
    "        for im, box_im, path_im in zip(img, batch_boxes, save_path):\n",
    "            if box_im is None:\n",
    "                faces.append(None)\n",
    "                continue\n",
    "\n",
    "            if not self.keep_all:\n",
    "                box_im = box_im[[0]]\n",
    "\n",
    "            faces_im = []\n",
    "            for i, box in enumerate(box_im):\n",
    "                face_path = path_im\n",
    "                if path_im is not None and i > 0:\n",
    "                    save_name, ext = os.path.splitext(path_im)\n",
    "                    face_path = save_name + '_' + str(i + 1) + ext\n",
    "\n",
    "                face = extract_face(im, box, self.image_size, self.margin, face_path)\n",
    "                if self.post_process:\n",
    "                    face = fixed_image_standardization(face)\n",
    "                faces_im.append(face)\n",
    "\n",
    "            if self.keep_all:\n",
    "                faces_im = torch.stack(faces_im)\n",
    "            else:\n",
    "                faces_im = faces_im[0]\n",
    "\n",
    "            faces.append(faces_im)\n",
    "\n",
    "        if not batch_mode:\n",
    "            faces = faces[0]\n",
    "\n",
    "        return faces\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Determine if an nvidia GPU is available"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print('Running on device: {}'.format(device))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "mtcnn = MTCNN_w_batch_extract(\n",
    "    image_size=160,\n",
    "    margin=32,\n",
    "    min_face_size=20,\n",
    "    thresholds=[0.6, 0.7, 0.7],\n",
    "    factor=0.709,\n",
    "    post_process=True,\n",
    "    select_largest=False,\n",
    "    device=device\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "#Define the data loader for the input set of images\n",
    "orig_img_ds = datasets.ImageFolder(data_dir, loader=exif_rotate_pil_loader, transform=transforms.Resize((1024, 1024)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "\n",
    "#overwrites class labels in dataset with path so path can be used for saving output in mtcnn batches\n",
    "orig_img_ds.samples = [\n",
    "    (p, p)\n",
    "    for p, _ in orig_img_ds.samples\n",
    "]\n",
    "\n",
    "loader = DataLoader(\n",
    "    orig_img_ds,\n",
    "    num_workers=workers,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=training.collate_pil\n",
    ")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zack/miniconda3/envs/pytorch/lib/python3.7/site-packages/facenet_pytorch/models/utils/detect_face.py:183: UserWarning: This overload of nonzero is deprecated:\n",
      "\tnonzero()\n",
      "Consider using one of the following signatures instead:\n",
      "\tnonzero(*, bool as_tuple) (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629403081/work/torch/csrc/utils/python_arg_parser.cpp:766.)\n",
      "  mask_inds = mask.nonzero()\n",
      "/home/zack/miniconda3/envs/pytorch/lib/python3.7/site-packages/facenet_pytorch/models/utils/detect_face.py:156: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  batch_boxes, batch_points = np.array(batch_boxes), np.array(batch_points)\n",
      "/home/zack/miniconda3/envs/pytorch/lib/python3.7/site-packages/facenet_pytorch/models/mtcnn.py:372: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  boxes = np.array(boxes)\n",
      "/home/zack/miniconda3/envs/pytorch/lib/python3.7/site-packages/facenet_pytorch/models/mtcnn.py:373: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  probs = np.array(probs)\n",
      "/home/zack/miniconda3/envs/pytorch/lib/python3.7/site-packages/facenet_pytorch/models/mtcnn.py:374: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  points = np.array(points)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 828 of 828"
     ]
    }
   ],
   "source": [
    "boxes = []\n",
    "box_probs = []\n",
    "paths = []\n",
    "\n",
    "for i, (x, b_paths) in enumerate(loader):\n",
    "    crops = [p.replace(data_dir, data_dir + '_cropped') for p in b_paths]\n",
    "    #crop_paths.append(crops)\n",
    "    #for now, doing two forward passes. One for detection and one for extraction. #TODO: make custom MTCNN class that has option for both\n",
    "    b_boxes, b_box_probs, points = mtcnn.detect(x, landmarks=True)\n",
    "    b_boxes, b_box_probs, points = mtcnn.select_boxes(b_boxes, b_box_probs, points, method='largest_over_threshold')\n",
    "    mtcnn.extract(x, b_boxes, save_path=crops)\n",
    "\n",
    "    boxes.extend(b_boxes)\n",
    "    box_probs.extend(b_box_probs)\n",
    "    paths.extend(b_paths)\n",
    "\n",
    "    print('\\rBatch {} of {}'.format(i + 1, len(loader)), end='')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "#Remove mtcnn to reduce GPU memory usage\n",
    "del mtcnn\n",
    "torch.cuda.empty_cache()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "#create dataset and data loaders from cropped images output from MTCNN\n",
    "\n",
    "trans = transforms.Compose([\n",
    "    np.float32,\n",
    "    transforms.ToTensor(),\n",
    "    fixed_image_standardization\n",
    "])\n",
    "\n",
    "#Training set can be much smaller because we aren't actually training, just creating a \"template\" in the gallery\n",
    "dataset = datasets.ImageFolder(data_dir + '_cropped', transform=trans)\n",
    "img_inds = np.arange(len(dataset))\n",
    "np.random.shuffle(img_inds)\n",
    "train_inds = img_inds[:int(0.3 * len(img_inds))]\n",
    "val_inds = img_inds[int(0.3 * len(img_inds)):]\n",
    "\n",
    "classes = dataset.classes\n",
    "\n",
    "#no need to randomize. there will be only one epoch. Basically don't need the dataloader except for batch control\n",
    "embed_loader = DataLoader(\n",
    "    dataset,\n",
    "    num_workers=workers,\n",
    "    batch_size=batch_size,\n",
    "    sampler=SequentialSampler(dataset)\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "#Load pretrained resnet model\n",
    "resnet = InceptionResnetV1(\n",
    "    classify=False,\n",
    "    pretrained='vggface2'\n",
    ").to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "classes = []\n",
    "embeddings = []\n",
    "resnet.eval()\n",
    "with torch.no_grad():\n",
    "    for xb, yb in embed_loader:\n",
    "        xb = xb.to(device)\n",
    "        b_embeddings = resnet(xb)\n",
    "        b_embeddings = b_embeddings.to('cpu').numpy()\n",
    "        classes.extend(yb.numpy())\n",
    "        embeddings.extend(b_embeddings)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "crop_paths = [p.replace(data_dir, data_dir + '_cropped') for p in paths]\n",
    "embeddings_dict = dict(zip(crop_paths,embeddings))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "#LFW functions taken from David Sandberg's FaceNet implementation\n",
    "from sklearn.model_selection import KFold\n",
    "from scipy import interpolate\n",
    "\n",
    "def distance(embeddings1, embeddings2, distance_metric=0):\n",
    "    if distance_metric==0:\n",
    "        # Euclidian distance\n",
    "        diff = np.subtract(embeddings1, embeddings2)\n",
    "        dist = np.sum(np.square(diff),1)\n",
    "    elif distance_metric==1:\n",
    "        # Distance based on cosine similarity\n",
    "        dot = np.sum(np.multiply(embeddings1, embeddings2), axis=1)\n",
    "        norm = np.linalg.norm(embeddings1, axis=1) * np.linalg.norm(embeddings2, axis=1)\n",
    "        similarity = dot / norm\n",
    "        dist = np.arccos(similarity) / math.pi\n",
    "    else:\n",
    "        raise 'Undefined distance metric %d' % distance_metric\n",
    "\n",
    "    return dist\n",
    "\n",
    "def calculate_roc(thresholds, embeddings1, embeddings2, actual_issame, nrof_folds=10, distance_metric=0, subtract_mean=False):\n",
    "    assert(embeddings1.shape[0] == embeddings2.shape[0])\n",
    "    assert(embeddings1.shape[1] == embeddings2.shape[1])\n",
    "    nrof_pairs = min(len(actual_issame), embeddings1.shape[0])\n",
    "    nrof_thresholds = len(thresholds)\n",
    "    k_fold = KFold(n_splits=nrof_folds, shuffle=False)\n",
    "\n",
    "    tprs = np.zeros((nrof_folds,nrof_thresholds))\n",
    "    fprs = np.zeros((nrof_folds,nrof_thresholds))\n",
    "    accuracy = np.zeros((nrof_folds))\n",
    "\n",
    "    indices = np.arange(nrof_pairs)\n",
    "\n",
    "    for fold_idx, (train_set, test_set) in enumerate(k_fold.split(indices)):\n",
    "        if subtract_mean:\n",
    "            mean = np.mean(np.concatenate([embeddings1[train_set], embeddings2[train_set]]), axis=0)\n",
    "        else:\n",
    "          mean = 0.0\n",
    "        dist = distance(embeddings1-mean, embeddings2-mean, distance_metric)\n",
    "\n",
    "        # Find the best threshold for the fold\n",
    "        acc_train = np.zeros((nrof_thresholds))\n",
    "        for threshold_idx, threshold in enumerate(thresholds):\n",
    "            _, _, acc_train[threshold_idx] = calculate_accuracy(threshold, dist[train_set], actual_issame[train_set])\n",
    "        best_threshold_index = np.argmax(acc_train)\n",
    "        for threshold_idx, threshold in enumerate(thresholds):\n",
    "            tprs[fold_idx,threshold_idx], fprs[fold_idx,threshold_idx], _ = calculate_accuracy(threshold, dist[test_set], actual_issame[test_set])\n",
    "        _, _, accuracy[fold_idx] = calculate_accuracy(thresholds[best_threshold_index], dist[test_set], actual_issame[test_set])\n",
    "\n",
    "        tpr = np.mean(tprs,0)\n",
    "        fpr = np.mean(fprs,0)\n",
    "    return tpr, fpr, accuracy\n",
    "\n",
    "def calculate_accuracy(threshold, dist, actual_issame):\n",
    "    predict_issame = np.less(dist, threshold)\n",
    "    tp = np.sum(np.logical_and(predict_issame, actual_issame))\n",
    "    fp = np.sum(np.logical_and(predict_issame, np.logical_not(actual_issame)))\n",
    "    tn = np.sum(np.logical_and(np.logical_not(predict_issame), np.logical_not(actual_issame)))\n",
    "    fn = np.sum(np.logical_and(np.logical_not(predict_issame), actual_issame))\n",
    "\n",
    "    tpr = 0 if (tp+fn==0) else float(tp) / float(tp+fn)\n",
    "    fpr = 0 if (fp+tn==0) else float(fp) / float(fp+tn)\n",
    "    acc = float(tp+tn)/dist.size\n",
    "    return tpr, fpr, acc\n",
    "\n",
    "def calculate_val(thresholds, embeddings1, embeddings2, actual_issame, far_target, nrof_folds=10, distance_metric=0, subtract_mean=False):\n",
    "    assert(embeddings1.shape[0] == embeddings2.shape[0])\n",
    "    assert(embeddings1.shape[1] == embeddings2.shape[1])\n",
    "    nrof_pairs = min(len(actual_issame), embeddings1.shape[0])\n",
    "    nrof_thresholds = len(thresholds)\n",
    "    k_fold = KFold(n_splits=nrof_folds, shuffle=False)\n",
    "\n",
    "    val = np.zeros(nrof_folds)\n",
    "    far = np.zeros(nrof_folds)\n",
    "\n",
    "    indices = np.arange(nrof_pairs)\n",
    "\n",
    "    for fold_idx, (train_set, test_set) in enumerate(k_fold.split(indices)):\n",
    "        if subtract_mean:\n",
    "            mean = np.mean(np.concatenate([embeddings1[train_set], embeddings2[train_set]]), axis=0)\n",
    "        else:\n",
    "          mean = 0.0\n",
    "        dist = distance(embeddings1-mean, embeddings2-mean, distance_metric)\n",
    "\n",
    "        # Find the threshold that gives FAR = far_target\n",
    "        far_train = np.zeros(nrof_thresholds)\n",
    "        for threshold_idx, threshold in enumerate(thresholds):\n",
    "            _, far_train[threshold_idx] = calculate_val_far(threshold, dist[train_set], actual_issame[train_set])\n",
    "        if np.max(far_train)>=far_target:\n",
    "            f = interpolate.interp1d(far_train, thresholds, kind='slinear')\n",
    "            threshold = f(far_target)\n",
    "        else:\n",
    "            threshold = 0.0\n",
    "\n",
    "        val[fold_idx], far[fold_idx] = calculate_val_far(threshold, dist[test_set], actual_issame[test_set])\n",
    "\n",
    "    val_mean = np.mean(val)\n",
    "    far_mean = np.mean(far)\n",
    "    val_std = np.std(val)\n",
    "    return val_mean, val_std, far_mean\n",
    "\n",
    "def calculate_val_far(threshold, dist, actual_issame):\n",
    "    predict_issame = np.less(dist, threshold)\n",
    "    true_accept = np.sum(np.logical_and(predict_issame, actual_issame))\n",
    "    false_accept = np.sum(np.logical_and(predict_issame, np.logical_not(actual_issame)))\n",
    "    n_same = np.sum(actual_issame)\n",
    "    n_diff = np.sum(np.logical_not(actual_issame))\n",
    "    val = float(true_accept) / float(n_same)\n",
    "    far = float(false_accept) / float(n_diff)\n",
    "    return val, far\n",
    "\n",
    "\n",
    "\n",
    "def evaluate(embeddings, actual_issame, nrof_folds=10, distance_metric=0, subtract_mean=False):\n",
    "    # Calculate evaluation metrics\n",
    "    thresholds = np.arange(0, 4, 0.01)\n",
    "    embeddings1 = embeddings[0::2]\n",
    "    embeddings2 = embeddings[1::2]\n",
    "    tpr, fpr, accuracy = calculate_roc(thresholds, embeddings1, embeddings2,\n",
    "        np.asarray(actual_issame), nrof_folds=nrof_folds, distance_metric=distance_metric, subtract_mean=subtract_mean)\n",
    "    thresholds = np.arange(0, 4, 0.001)\n",
    "    val, val_std, far = calculate_val(thresholds, embeddings1, embeddings2,\n",
    "        np.asarray(actual_issame), 1e-3, nrof_folds=nrof_folds, distance_metric=distance_metric, subtract_mean=subtract_mean)\n",
    "    return tpr, fpr, accuracy, val, val_std, far\n",
    "\n",
    "def add_extension(path):\n",
    "    if os.path.exists(path+'.jpg'):\n",
    "        return path+'.jpg'\n",
    "    elif os.path.exists(path+'.png'):\n",
    "        return path+'.png'\n",
    "    else:\n",
    "        raise RuntimeError('No file \"%s\" with extension png or jpg.' % path)\n",
    "\n",
    "def get_paths(lfw_dir, pairs):\n",
    "    nrof_skipped_pairs = 0\n",
    "    path_list = []\n",
    "    issame_list = []\n",
    "    for pair in pairs:\n",
    "        if len(pair) == 3:\n",
    "            path0 = add_extension(os.path.join(lfw_dir, pair[0], pair[0] + '_' + '%04d' % int(pair[1])))\n",
    "            path1 = add_extension(os.path.join(lfw_dir, pair[0], pair[0] + '_' + '%04d' % int(pair[2])))\n",
    "            issame = True\n",
    "        elif len(pair) == 4:\n",
    "            path0 = add_extension(os.path.join(lfw_dir, pair[0], pair[0] + '_' + '%04d' % int(pair[1])))\n",
    "            path1 = add_extension(os.path.join(lfw_dir, pair[2], pair[2] + '_' + '%04d' % int(pair[3])))\n",
    "            issame = False\n",
    "        if os.path.exists(path0) and os.path.exists(path1):    # Only add the pair if both paths exist\n",
    "            path_list += (path0,path1)\n",
    "            issame_list.append(issame)\n",
    "        else:\n",
    "            nrof_skipped_pairs += 1\n",
    "    if nrof_skipped_pairs>0:\n",
    "        print('Skipped %d image pairs' % nrof_skipped_pairs)\n",
    "\n",
    "    return path_list, issame_list\n",
    "\n",
    "def read_pairs(pairs_filename):\n",
    "    pairs = []\n",
    "    with open(pairs_filename, 'r') as f:\n",
    "        for line in f.readlines()[1:]:\n",
    "            pair = line.strip().split()\n",
    "            pairs.append(pair)\n",
    "    return np.array(pairs, dtype=object)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "pairs = read_pairs(pairs_path)\n",
    "path_list, issame_list = get_paths(data_dir+'_cropped', pairs)\n",
    "embeddings = np.array([embeddings_dict[path] for path in path_list])\n",
    "\n",
    "tpr, fpr, accuracy, val, val_std, far = evaluate(embeddings, issame_list)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.97666667 0.96666667 0.95666667 0.98       0.97666667 0.975\n",
      " 0.98       0.97166667 0.97       0.975     ]\n",
      "0.9728333333333333\n"
     ]
    }
   ],
   "source": [
    "print(accuracy)\n",
    "print(np.mean(accuracy))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Notes\n",
    "no big difference in accuracy using deep funneling aligned LFW images\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}