{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using FaceNet with distance metrics\n",
    "\n",
    "Finetuning works well enough for a closed set problem, but won't for open set which is my eventual use case. From what I've seen the standard protocol for benchmarking is to perform classification or verification from the embeddings directly. The benchmarks I've seen are on a verification task. So evaluation involves labeling two photos as the same identity or not. In this case, a simple threshold for a similarity score (euclidean distance or cosine similarity depending on the loss in the original training).\n",
    "\n",
    "So this notebook will look at how to use Euclidean distance with FaceNet for classification starting with the same closed set I used for finetuning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from facenet_pytorch import MTCNN, InceptionResnetV1, fixed_image_standardization, training, extract_face\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler, SequentialSampler\n",
    "from torch import optim\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import torch.nn.functional as F\n",
    "from copy import deepcopy\n",
    "import shutil\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data_dir = 'data/zack_erin'\n",
    "\n",
    "batch_size = 16\n",
    "epochs = 15\n",
    "workers = 0 if os.name == 'nt' else 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Needed to rotate files based on exif. Default loader strips exif, so can't do in transform step. PIL files don't keep exif when made from MPO file\n",
    "\n",
    "def exif_rotate_pil_loader(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        image = Image.open(f)\n",
    "        image = reorient_image(image)\n",
    "        image = image.convert('RGB')  #replicates pil_loader from torchvision. Copies to convert to PIL\n",
    "    return image\n",
    "\n",
    "\n",
    "def reorient_image(im):\n",
    "    try:\n",
    "        image_exif = im._getexif()\n",
    "        image_orientation = image_exif[274]\n",
    "        if image_orientation in (2, '2'):\n",
    "            return im.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "        elif image_orientation in (3, '3'):\n",
    "            return im.transpose(Image.ROTATE_180)\n",
    "        elif image_orientation in (4, '4'):\n",
    "            return im.transpose(Image.FLIP_TOP_BOTTOM)\n",
    "        elif image_orientation in (5, '5'):\n",
    "            return im.transpose(Image.ROTATE_90).transpose(Image.FLIP_TOP_BOTTOM)\n",
    "        elif image_orientation in (6, '6'):\n",
    "            return im.transpose(Image.ROTATE_270)\n",
    "        elif image_orientation in (7, '7'):\n",
    "            return im.transpose(Image.ROTATE_270).transpose(Image.FLIP_TOP_BOTTOM)\n",
    "        elif image_orientation in (8, '8'):\n",
    "            return im.transpose(Image.ROTATE_90)\n",
    "        else:\n",
    "            return im\n",
    "    except (KeyError, AttributeError, TypeError, IndexError):\n",
    "        return im\n",
    "\n",
    "\n",
    "def matplotlib_imshow(img, one_channel=False):\n",
    "    if one_channel:\n",
    "        img = img.mean(dim=0)\n",
    "    img = img / 2 + 0.5  #unnormalize\n",
    "    npimg = img.cpu().numpy()\n",
    "    if one_channel:\n",
    "        plt.imshow(npimg, cmap=\"Greys\")\n",
    "    else:\n",
    "        plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "\n",
    "\n",
    "def plot_classes_preds(preds, images, labels, classes, fig, save_path=None):\n",
    "    '''\n",
    "    Generates matplotlib Figure using a trained network, along with images\n",
    "    and labels from a batch, that shows the network's top prediction along\n",
    "    with its probability, alongside the actual label, coloring this\n",
    "    information based on whether the prediction was correct or not.\n",
    "    Uses the \"images_to_probs\" function.\n",
    "    '''\n",
    "    #plot the images in the batch, along with predicted and true labels\n",
    "    #if not fig:\n",
    "    fig = plt.figure(figsize=(20, 7))\n",
    "    for idx in np.arange(len(preds)):\n",
    "        ax = fig.add_subplot(2, np.ceil(len(preds)/2), idx+1, xticks=[], yticks=[])\n",
    "        matplotlib_imshow(images[idx], one_channel=False)\n",
    "        ax.set_title(\"{0}, {1:.1f}%\\n(label: {2})\".format(\n",
    "            classes[preds[idx]],\n",
    "            classes[labels[idx]]),\n",
    "            color=(\"green\" if preds[idx] == labels[idx].item() else \"red\"))\n",
    "    if save_path:\n",
    "        plt.savefig(save_path)\n",
    "    else:\n",
    "        return fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class MTCNN_w_batch_extract(MTCNN):\n",
    "    \"\"\"custom class that includes addtional methods to allow for easier separation of detection,\n",
    "        selection and extraction.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def select_boxes(self, all_boxes, all_probs, all_points, method='probability', threshold=.9):\n",
    "        \"\"\"Selects a single box from multiple for a given image using one of multiple heuristics.\n",
    "        Arguments:\n",
    "                batch_boxes {np.ndarray} -- Nx4 ndarray of bounding boxes for N detected faces (output from self.detect)\n",
    "                batch_probs {list} -- Length N list of probalities for N detected faces. (output from self.detect)\n",
    "        Keyword Arguments:\n",
    "                method {str} -- Which heuristic to use for selection:\n",
    "                    \"probability\": highest probability selected\n",
    "                    \"largest\": largest box selected\n",
    "                    \"largest_over_theshold\": largest box over a certain probability threshold selected\n",
    "                threshold {float} -- theshold for \"largest_over_threshold\" method\n",
    "\n",
    "        Returns:\n",
    "                tuple(numpy.ndarray, numpy.ndarray) -- Ix4 ndarray of bounding boxes for I images. Ix0 array of probabilities for each box\n",
    "        \"\"\"\n",
    "        selected_boxes, selected_probs, selected_points = [], [], []\n",
    "        for boxes, points, probs in zip(all_boxes, all_points, all_probs):\n",
    "            boxes = np.array(boxes)\n",
    "            probs = np.array(probs)\n",
    "            points = np.array(points)\n",
    "            if len(boxes) == 0:\n",
    "                selected_boxes.append(None)\n",
    "                selected_probs.append([None])\n",
    "                selected_points.append(None)\n",
    "                continue\n",
    "            elif method == 'largest':\n",
    "                box_order = np.argsort((boxes[:, 2] - boxes[:, 0]) * (boxes[:, 3] - boxes[:, 1]))[::-1]\n",
    "            elif method == 'probability':\n",
    "                box_order = np.argsort(probs)[::-1]\n",
    "            elif method == 'largest_over_threshold':\n",
    "                box_mask = probs > threshold\n",
    "                boxes = boxes[box_mask]\n",
    "                box_order = np.argsort((boxes[:, 2] - boxes[:, 0]) * (boxes[:, 3] - boxes[:, 1]))[::-1]\n",
    "                if sum(box_mask) == 0:\n",
    "                    selected_boxes.append(None)\n",
    "                    selected_probs.append([None])\n",
    "                    selected_points.append(None)\n",
    "                    continue\n",
    "\n",
    "            box = boxes[box_order][[0]]\n",
    "            prob = probs[box_order][[0]]\n",
    "            point = points[box_order][[0]]\n",
    "            selected_boxes.append(box)\n",
    "            selected_probs.append(prob)\n",
    "            selected_points.append(point)\n",
    "\n",
    "        selected_boxes = np.array(selected_boxes)\n",
    "        selected_probs = np.array(selected_probs)\n",
    "        selected_points = np.array(selected_points)\n",
    "\n",
    "        return selected_boxes, selected_probs, selected_points\n",
    "\n",
    "    def extract(self, img, batch_boxes, save_path):\n",
    "        #Determine if a batch or single image was passed\n",
    "        batch_mode = True\n",
    "        if not isinstance(img, (list, tuple)) and not (isinstance(img, np.ndarray) and len(img.shape) == 4):\n",
    "            img = [img]\n",
    "            batch_boxes = [batch_boxes]\n",
    "            batch_mode = False\n",
    "\n",
    "        #Parse save path(s)\n",
    "        if save_path is not None:\n",
    "            if isinstance(save_path, str):\n",
    "                save_path = [save_path]\n",
    "        else:\n",
    "            save_path = [None for _ in range(len(img))]\n",
    "\n",
    "        #Process all bounding boxes\n",
    "        faces = []\n",
    "        for im, box_im, path_im in zip(img, batch_boxes, save_path):\n",
    "            if box_im is None:\n",
    "                faces.append(None)\n",
    "                continue\n",
    "\n",
    "            if not self.keep_all:\n",
    "                box_im = box_im[[0]]\n",
    "\n",
    "            faces_im = []\n",
    "            for i, box in enumerate(box_im):\n",
    "                face_path = path_im\n",
    "                if path_im is not None and i > 0:\n",
    "                    save_name, ext = os.path.splitext(path_im)\n",
    "                    face_path = save_name + '_' + str(i + 1) + ext\n",
    "\n",
    "                face = extract_face(im, box, self.image_size, self.margin, face_path)\n",
    "                if self.post_process:\n",
    "                    face = fixed_image_standardization(face)\n",
    "                faces_im.append(face)\n",
    "\n",
    "            if self.keep_all:\n",
    "                faces_im = torch.stack(faces_im)\n",
    "            else:\n",
    "                faces_im = faces_im[0]\n",
    "\n",
    "            faces.append(faces_im)\n",
    "\n",
    "        if not batch_mode:\n",
    "            faces = faces[0]\n",
    "\n",
    "        return faces\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Determine if an nvidia GPU is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print('Running on device: {}'.format(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "mtcnn = MTCNN_w_batch_extract(\n",
    "    image_size=160,\n",
    "    margin=30,\n",
    "    min_face_size=20,\n",
    "    thresholds=[0.6, 0.7, 0.7],\n",
    "    factor=0.709,\n",
    "    post_process=True,\n",
    "    select_largest=False,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Define the data loader for the input set of images\n",
    "orig_img_ds = datasets.ImageFolder(data_dir, loader=exif_rotate_pil_loader, transform=transforms.Resize((1024, 1024)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "#overwrites class labels in dataset with path so path can be used for saving output in mtcnn batches\n",
    "orig_img_ds.samples = [\n",
    "    (p, p)\n",
    "    for p, _ in orig_img_ds.samples\n",
    "]\n",
    "\n",
    "loader = DataLoader(\n",
    "    orig_img_ds,\n",
    "    num_workers=workers,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=training.collate_pil\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zack/miniconda3/envs/pytorch/lib/python3.7/site-packages/facenet_pytorch/models/utils/detect_face.py:183: UserWarning: This overload of nonzero is deprecated:\n",
      "\tnonzero()\n",
      "Consider using one of the following signatures instead:\n",
      "\tnonzero(*, bool as_tuple) (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629403081/work/torch/csrc/utils/python_arg_parser.cpp:766.)\n",
      "  mask_inds = mask.nonzero()\n",
      "/home/zack/miniconda3/envs/pytorch/lib/python3.7/site-packages/facenet_pytorch/models/utils/detect_face.py:156: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  batch_boxes, batch_points = np.array(batch_boxes), np.array(batch_points)\n",
      "/home/zack/miniconda3/envs/pytorch/lib/python3.7/site-packages/facenet_pytorch/models/mtcnn.py:372: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  boxes = np.array(boxes)\n",
      "/home/zack/miniconda3/envs/pytorch/lib/python3.7/site-packages/facenet_pytorch/models/mtcnn.py:373: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  probs = np.array(probs)\n",
      "/home/zack/miniconda3/envs/pytorch/lib/python3.7/site-packages/facenet_pytorch/models/mtcnn.py:374: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  points = np.array(points)\n",
      "/home/zack/miniconda3/envs/pytorch/lib/python3.7/site-packages/ipykernel_launcher.py:55: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "/home/zack/miniconda3/envs/pytorch/lib/python3.7/site-packages/ipykernel_launcher.py:57: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 12 of 12"
     ]
    }
   ],
   "source": [
    "boxes = []\n",
    "box_probs = []\n",
    "paths = []\n",
    "for i, (x, b_paths) in enumerate(loader):\n",
    "    crops = [p.replace(data_dir, data_dir + '_cropped') for p in b_paths]\n",
    "    #for now, doing two forward passes. One for detection and one for extraction. #TODO: make custom MTCNN class that has option for both\n",
    "    b_boxes, b_box_probs, points = mtcnn.detect(x, landmarks=True)\n",
    "    b_boxes, b_box_probs, points = mtcnn.select_boxes(b_boxes, b_box_probs, points, method='largest_over_threshold')\n",
    "    mtcnn.extract(x, b_boxes, save_path=crops)\n",
    "\n",
    "    boxes.extend(b_boxes)\n",
    "    box_probs.extend(b_box_probs)\n",
    "    paths.extend(b_paths)\n",
    "\n",
    "    print('\\rBatch {} of {}'.format(i + 1, len(loader)), end='')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Remove mtcnn to reduce GPU memory usage\n",
    "del mtcnn\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#create dataset and data loaders from cropped images output from MTCNN\n",
    "\n",
    "trans = transforms.Compose([\n",
    "    np.float32,\n",
    "    transforms.ToTensor(),\n",
    "    fixed_image_standardization\n",
    "])\n",
    "\n",
    "#Training set can be much smaller because we aren't actually training, just creating a \"template\" in the gallery\n",
    "dataset = datasets.ImageFolder(data_dir + '_cropped', transform=trans)\n",
    "img_inds = np.arange(len(dataset))\n",
    "np.random.shuffle(img_inds)\n",
    "train_inds = img_inds[:int(0.3 * len(img_inds))]\n",
    "val_inds = img_inds[int(0.3 * len(img_inds)):]\n",
    "\n",
    "classes = dataset.classes\n",
    "\n",
    "#no need to randomize. there will be only one epoch. Basically don't need the dataloader except for batch control\n",
    "embed_loader = DataLoader(\n",
    "    dataset,\n",
    "    num_workers=workers,\n",
    "    batch_size=batch_size,\n",
    "    sampler=SequentialSampler(dataset)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Load pretrained resnet model\n",
    "resnet = InceptionResnetV1(\n",
    "    classify=False,\n",
    "    pretrained='vggface2'\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "classes = []\n",
    "embeddings = []\n",
    "with torch.no_grad():\n",
    "    for xb, yb in embed_loader:\n",
    "        xb = xb.to(device)\n",
    "        b_embeddings = resnet(xb)\n",
    "        b_embeddings = b_embeddings.to('cpu').numpy()\n",
    "        classes.extend(yb.numpy())\n",
    "        embeddings.extend(b_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.851063829787234\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.851063829787234"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score, cross_val_predict, train_test_split\n",
    "from sklearn.neighbors import RadiusNeighborsClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "\n",
    "train_embeddings, val_embeddings, train_classes, val_classes = map(np.array, train_test_split(embeddings, classes))\n",
    "\n",
    "#think this is basically the same as radius neighbors, but with majority requirement, but I'll see how similar they are\n",
    "def threshold_nearest_neighbor(val_embedding, train_embeddings, train_classes, threshold, majority_req):\n",
    "    distances = np.linalg.norm(train_embeddings - val_embedding, axis=1)\n",
    "    neighbors = np.array(train_classes)[distances<threshold]\n",
    "    if len(neighbors) == 0:\n",
    "        return 2\n",
    "    votes = np.unique(neighbors, return_counts=True)\n",
    "    if majority_req and max(votes[1]) < int(len(neighbors/2)):\n",
    "        return 2\n",
    "    highest_votes = votes[0][np.argmax(votes[1])]\n",
    "    return highest_votes\n",
    "\n",
    "val_preds = []\n",
    "for val_embedding in val_embeddings:\n",
    "    val_preds.append(threshold_nearest_neighbor(val_embedding, train_embeddings, train_classes, 1.15, False))\n",
    "\n",
    "accuracy = np.sum(np.equal(val_preds, val_classes))/len(val_classes)\n",
    "print(accuracy)\n",
    "accuracy_score(val_classes,val_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.73684211 0.71052632 0.7027027  0.7027027  0.72972973]\n",
      "[0 0 2 0 0 0 0 2 0 0 0 0 2 0 1 0 1 0 0 0 0 0 0 0 0 1 0 2 1 1 0 0 0 0 0 0 0\n",
      " 0 0 1 1 1 0 0 2 0 1 0 0 0 0 1 1 0 0 0 0 0 2 0 0 0 0 0 0 1 0 0 0 0 0 0 0 2\n",
      " 1 2 0 2 2 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 1 1 1 1 1 1 0 1 0 0 1 1 0 0 1 1 0\n",
      " 1 0 1 0 1 1 0 1 1 1 1 1 1 0 1 0 1 1 1 1 1 1 1 1 1 0 0 1 1 1 0 0 1 1 1 1 1\n",
      " 0 1 1 0 1 1 1 1 1 0 1 0 0 0 1 0 2 1 1 1 0 0 1 1 0 1 0 2 1 2 1 1 1 1 2 1 0\n",
      " 1 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zack/miniconda3/envs/pytorch/lib/python3.7/site-packages/sklearn/neighbors/_classification.py:571: UserWarning: Outlier label 2 is not in training classes. All class probabilities of outliers will be assigned with 0.\n",
      "  ''.format(self.outlier_label_[k]))\n",
      "/home/zack/miniconda3/envs/pytorch/lib/python3.7/site-packages/sklearn/neighbors/_classification.py:571: UserWarning: Outlier label 2 is not in training classes. All class probabilities of outliers will be assigned with 0.\n",
      "  ''.format(self.outlier_label_[k]))\n",
      "/home/zack/miniconda3/envs/pytorch/lib/python3.7/site-packages/sklearn/neighbors/_classification.py:571: UserWarning: Outlier label 2 is not in training classes. All class probabilities of outliers will be assigned with 0.\n",
      "  ''.format(self.outlier_label_[k]))\n",
      "/home/zack/miniconda3/envs/pytorch/lib/python3.7/site-packages/sklearn/neighbors/_classification.py:571: UserWarning: Outlier label 2 is not in training classes. All class probabilities of outliers will be assigned with 0.\n",
      "  ''.format(self.outlier_label_[k]))\n",
      "/home/zack/miniconda3/envs/pytorch/lib/python3.7/site-packages/sklearn/neighbors/_classification.py:571: UserWarning: Outlier label 2 is not in training classes. All class probabilities of outliers will be assigned with 0.\n",
      "  ''.format(self.outlier_label_[k]))\n",
      "/home/zack/miniconda3/envs/pytorch/lib/python3.7/site-packages/sklearn/neighbors/_classification.py:571: UserWarning: Outlier label 2 is not in training classes. All class probabilities of outliers will be assigned with 0.\n",
      "  ''.format(self.outlier_label_[k]))\n",
      "/home/zack/miniconda3/envs/pytorch/lib/python3.7/site-packages/sklearn/neighbors/_classification.py:571: UserWarning: Outlier label 2 is not in training classes. All class probabilities of outliers will be assigned with 0.\n",
      "  ''.format(self.outlier_label_[k]))\n",
      "/home/zack/miniconda3/envs/pytorch/lib/python3.7/site-packages/sklearn/neighbors/_classification.py:571: UserWarning: Outlier label 2 is not in training classes. All class probabilities of outliers will be assigned with 0.\n",
      "  ''.format(self.outlier_label_[k]))\n",
      "/home/zack/miniconda3/envs/pytorch/lib/python3.7/site-packages/sklearn/neighbors/_classification.py:571: UserWarning: Outlier label 2 is not in training classes. All class probabilities of outliers will be assigned with 0.\n",
      "  ''.format(self.outlier_label_[k]))\n",
      "/home/zack/miniconda3/envs/pytorch/lib/python3.7/site-packages/sklearn/neighbors/_classification.py:571: UserWarning: Outlier label 2 is not in training classes. All class probabilities of outliers will be assigned with 0.\n",
      "  ''.format(self.outlier_label_[k]))\n"
     ]
    }
   ],
   "source": [
    "cv = StratifiedKFold(5, shuffle=True)\n",
    "neighbors = RadiusNeighborsClassifier(radius=1.15, outlier_label=2)\n",
    "scores = cross_val_score(neighbors, embeddings, classes, cv=cv)\n",
    "preds = cross_val_predict(neighbors, embeddings, classes, cv=cv)\n",
    "print(scores)\n",
    "print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plot_classes_preds(val_preds,val_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes\n",
    "\n",
    "Accuracy is pretty terrible compared to adding a FC softmax layer and finetuning. I'm going to try out on LFW to see if this is dataset related.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
